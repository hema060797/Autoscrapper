{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Autoscrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autoscraper in c:\\users\\l\\anaconda3\\lib\\site-packages (1.1.12)\n",
      "Requirement already satisfied: bs4 in c:\\users\\l\\anaconda3\\lib\\site-packages (from autoscraper) (0.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\l\\anaconda3\\lib\\site-packages (from autoscraper) (2.24.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\l\\anaconda3\\lib\\site-packages (from autoscraper) (4.5.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\l\\anaconda3\\lib\\site-packages (from bs4->autoscraper) (4.9.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\l\\anaconda3\\lib\\site-packages (from requests->autoscraper) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\l\\anaconda3\\lib\\site-packages (from requests->autoscraper) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\l\\anaconda3\\lib\\site-packages (from requests->autoscraper) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\l\\anaconda3\\lib\\site-packages (from requests->autoscraper) (1.25.9)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\l\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4->autoscraper) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install autoscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datavisualization', 'Evalml', 'lazy-predict', 'Pywebio-heroku', 'PyWeb-IO', 'customersegmentation', 'Fast-Api', 'R-Programming', 'Feature-Engineering', 'Numpy', 'sqlite3', 'Stats_for_datascience', 'Restaurant_Management_System', 'Encoding_Techniques', 'Spellcheck-tkinter', 'Outlier', 'Data-Normalization-Standardization', 'ChatBot', 'Corona-EDA', 'NLP', 'Imputation', 'Learn-Python', 'Cheatsheet', 'PYTHON', 'Fake-News-Detection', 'Uni_Bi_multivariate-analysis', 'flight_price_prediction', 'Bank_authentication_note', 'Car-price-Prediction', 'Credit-card-fraud--detection', '3', '4', '0']\n"
     ]
    }
   ],
   "source": [
    "from autoscraper import AutoScraper\n",
    "\n",
    "url = 'https://github.com/hema060797?tab=repositories'\n",
    "\n",
    "\n",
    "# We can add one or multiple candidates here.\n",
    "# You can also put urls here to retrieve urls.\n",
    "wanted_list = [\"datavisualization\",\"4\"]\n",
    "\n",
    "scraper = AutoScraper()\n",
    "result = scraper.build(url, wanted_list)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rule_8wt0': ['datavisualization',\n",
       "  'Evalml',\n",
       "  'lazy-predict',\n",
       "  'Pywebio-heroku',\n",
       "  'PyWeb-IO',\n",
       "  'customersegmentation',\n",
       "  'Fast-Api',\n",
       "  'R-Programming',\n",
       "  'Feature-Engineering',\n",
       "  'Numpy',\n",
       "  'sqlite3',\n",
       "  'Stats_for_datascience',\n",
       "  'Restaurant_Management_System',\n",
       "  'Encoding_Techniques',\n",
       "  'Spellcheck-tkinter',\n",
       "  'Outlier',\n",
       "  'Data-Normalization-Standardization',\n",
       "  'ChatBot',\n",
       "  'Corona-EDA',\n",
       "  'NLP',\n",
       "  'Imputation',\n",
       "  'Learn-Python',\n",
       "  'Cheatsheet',\n",
       "  'PYTHON',\n",
       "  'Fake-News-Detection',\n",
       "  'Uni_Bi_multivariate-analysis',\n",
       "  'flight_price_prediction',\n",
       "  'Bank_authentication_note',\n",
       "  'Car-price-Prediction',\n",
       "  'Credit-card-fraud--detection'],\n",
       " 'rule_bd1j': ['3', '4', '0']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper.get_result_similar(\"https://github.com/hema060797?tab=repositories\",grouped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.set_rule_aliases({'rule_8wt0': 'Title','rule_bd1j':'Followers'})\n",
    "\n",
    "scraper.keep_rules(['rule_8wt0','rule_bd1j'])\n",
    "scraper.save('github-repository-search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.load('github-repository-search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=scraper.get_result_similar(\"https://github.com/krishnaik06?tab=repositories\",group_by_alias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8.4k', '0', '2']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['Followers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
